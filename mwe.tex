\documentclass[11pt,english,a4paper]{article}
\usepackage{babel}
\input{/home/marius/Dokumenter/preamples/phys_en.pre}
\usepackage{lstautogobble}
\usepackage{csquotes}
\author{\normalsize Marius Jonsson, Morten Hjorth-Jensen}
\title{\bf \uppercase{The automated blocking method}}
\date{\normalsize \today}
\addbibresource{/home/marius/Dokumenter/MyLibrary.bib}
\DeclareUnicodeCharacter{2212}{$-$}
\begin{document}
\maketitle
\begin{abstract} \normalsize
Suppose $\overline{ X}_n = (1/n) \sum_{i=1}^{n} X_i$ is the mean of $n = 2^d$ weakly stationary observations $X_1,X_2,\cdots,X_n$. Using $\leq 7n+4$ floating point operations, we find an accurate estimator the variance of the mean, $\Var{\overline{X}_n}$. And we provide estimate of an upper bound of the error of the algorithm. Suppose $\E{X}$ denote the expected value of the random variable $X$, then the variance of the mean is
\[
\Var{\overline{X}_n} = \frac{1}{n-1} \bigg[ \E{\what \sigma^2} + \sum_{i=0}^{d-1}2^i \gamma^{(i)}_1  \bigg],
\]
\[
\Var{\overline{X}_n} =  \frac{1 }{n^{k}}\Var{X^k_i} + \sum_{i=k}^{d-1} \frac{ \gamma^{(i)}_1 }{n^{k}}
\]
where $\gamma_1^{(i)}$ denote the autocovariance of the data subject to some 
transformation $( X_i , X_{i+1} ) \mapsto X_i^{(j)}$, which will be define. We propose 
an estimator and found that this automated algorithm is more accurate than 
previously proposed "blocking" methods, and is even more accurate than higher 
complexity methods, such as dependent bootstrapping.
\end{abstract}
\section*{\uppercase{Introduction}}
Estimation of the variance of the sample mean is important in all of the sciences. Since 1867 (ref) Chebyshev's inequality told us that the expected value, $\E{X}$ is well approximated by the sample mean $\overline{X}_n$. However, the variance of the mean is necessary to quantify the uncertainty in the estimate. And for a random sample, this is easily obtained, but for correlated data, the computation is more complicated. However, in this text, we show that if $X_1,X_2,\cdots,X_n$ are $n = 2^d$ weakly stationary observations, then the complexity is no larger than that of the sample mean.\\
\\
We will utilize so called blocking transformations. This refers to forming a new sample of data by taking the mean of every pair of subsequent observations. People has been making such transformations in all areas of mathematics, and (ref) Flydberg and Pedersen made popular a method where blocking transformations reduced the correlations of data, and proposed a way to estimate the variance of the mean. Our results contain a new algorithm with an improved estimator and estimate of the upper bound of the error. The improvements are improved accuracy, automation and there is rigorous proof throughout.\\
\\
The article is structured by «introduction»-, «methods»-, «results and discussion»- and finally a «conclusion and perspectives»-sections.
\section*{\uppercase{Methods}}
We will agree about the background
-Almost surely,
- * means conjugate
- bracket means expectation
- def: subject to k blocking transformations
\begin{equation}
\Var(A) = \E{A^2} - \E{A}^2, \qquad \cov(A,B) = \E{AB} - \E{A}\E{B} \label{eq:varcov}
\end{equation}
\begin{equation}
X^{(k+1)}_{i} = \frac{1}{2} \left( X^{(k)}_{2i-1} + X^{(k)}_{2i} \right) \label{eq:block}
\end{equation}
It is known that 
\begin{theorem}
If $X_i = \mu + \sum_{j=1}^{2^d}\psi_jw_{i-j}$ is a stationary linear process and there exist $\eta \sigma^2_w \in \mathbb{R}$ such that for all $0<i<2^d$ its fourth moment satisfies $\E{w_t^4} = \eta \sigma^2_w$ and $k < 2^d$, then $\what \rho_t$ is asymptotically multivariate normal distributed with expected value $\rho_t$ and covariance matrix $\Sigma$ with elements
\[
\Sigma_{pq} = 2^{-d} \sum_{u=1}^{2^d-1} \left[ \rho(u + p) + \rho(u-p) - 2 \rho(p) \rho(u) \right]\left[ \rho(u+q) + \rho(u-q) - 2\rho(q)\rho(u) \right].
\]
\label{prop:rho}
\end{theorem}
\begin{proof}
See Shumway and Stoffer 2011 or the appendix.
\end{proof}

\section*{\uppercase{Results}}
We first state the results, then prove them in the order they were introduced. Our main result is an automated algorithm of the blocking method, some preliminary results are required. 
\subsection*{\uppercase{Preliminary results}}
The most counterintuitive of which is the following. It tells us that the correlation between sequantial observations contain all the information that exist aout the rate of change of the error.
\begin{prop}
Suppose $2^d$ is the number of observations, and $\gamma^{(k)}_0$ is finite for all $k \in \{ 0,1,\cdots, d-1 \}$. Then the rate of change of the error is
\begin{equation}
\varepsilon_k - \varepsilon_{  k+1} = \frac{\gamma_1^{(k)} }{n^{(k)}} \qquad \text{for all} \qquad 0 \leq k < d-1 \label{eq:rate}.
\end{equation}
\label{prop:diff}
\end{prop}
Perhaps not surprisingly, the behaviour of he blocking method follows directly from this proposition. To be precise, we will prove that
\begin{corollary}
If $2^d$ is the number of observations, $i,j\in \mathbb{N}$ and $\gamma^{(k)}_0$ is finite for all $k \in \{ 0,1,\cdots, d-1 \}$;
\begin{enumerate}
\item if there exist $k \in \mathbb{N}$ such that $\gamma^{(k)}_1 > 0$ for all $i \leq k \leq j$, then the sequence of errors $\varepsilon_k$ is strictly decreasing for all $j \leq k \leq i$.
\item if there exist $k \in \mathbb{N}$ such that $\gamma^{(k)}_1 \geq 0$ for all $i \leq k \leq j$, then the sequence of errors $\varepsilon_k$ is nonstrictly decreasing for all $j \leq k \leq i$.
\item if there exist some $i \in \mathbb{N}$ such that $\gamma^{(i)}_1 = 0$, and $\gamma^{(i)}_t \geq \gamma^{(i)}_{t+1}$ then the sequence of errors $\varepsilon_k$ is constant for all $i \leq k < d-1$.
\end{enumerate}
\end{corollary}
\begin{corollary}
If the blocked variables become independent, for some number of blocking transformation, then the sequence of errors $\varepsilon_k$ become constant.
\end{corollary}
\begin{figure}[!h]
\begin{center}
\begin{lstlisting}[language=python]
# (1) initialization
n = len(x)
d = log2(n)
mu = mean(x)
s, gamma = zeros(d), zeros(d)

# (2) estimate the auto-covariance and variances 
#     for each blocking transformation
for i in range(0,d):
   n = len(x)
   # estimate autocovariance of x
   gamma[i] = n**(-1)*sum( (x[2:n]-mu)*(x[1:(n-1)]-mu) )
   # estimate variance of x
   s[i] = var(x)
   # perform blocking transformation
   x = zeros(n/2)
   for j in range(0, n/2+1):
      x[j] = 0.5*( x[2*j] + x[2*j+1] )
  
# (3) generate the test observator Z_k from the theorem      
Z = zeros(d)
for k in range(0,d):
   Z[k] = (gamma[k]/s[k])**2*(d-k)
Z = flip(cumsum(flip(Z)))

# (4) save vector of 99-percentiles of chi square
q = array([6.634897,9.210340, ... , 50.892181])

# (5) determine the smallest value k such that H_0 is true
for k in range(0,d):
   if(Z[k] < q[k]):
      break

# (6) the answer is ans
n = 2**(d-k)
ans = s[k]/n
\end{lstlisting}
\caption{A simple implementation for \texttt{python}. In step (1) we initialize some variables. \texttt{n} is the length of the ovservation vector \texttt{x} containing $X_1^{0}, X_2^{0}, \cdots, X_{2^d}^{0}$. \texttt{mu} is the mean of \texttt{x} which is required when we for each blocking iteration compute the auto-covariance \texttt{gamma}. The array \texttt{s} will contain the variances for each blocking iteration. In step (2), we for each blocking iteration \texttt{i} compute the associated \texttt{gamma[i]} and \texttt{s[i]} and perform a blocking transformation. In step (3) we cumulatively sum the test observator $Z_k$. In step (4) we save an array of at least length \texttt{d} which contains all the quantiles from the chi square distribution. In step (5) we perform the hypothesis test to determine the appropriate $k$ and finally (6) compute the answer at the iteration number $k$.}
\end{center}
\end{figure}
\subsection*{\uppercase{Algorithm}}
By the proposition, we know that the rate of change of the error is given by $\gamma_1^{(n)}$. That means that if there exist some $k \in \mathbb{N}$ such that for all $n > k$, then $\gamma_1^{(n)}$ is not significantly different from zero, then we stop the algorithm, because iterating further has no benefit (since by the colollary, the estimate cannot get any better). Actually, readers whom are familliar with the blocking method knows that iterating to far actually induces estra errors since we are not dealing with the $\gamma_i^{(n)}$s directly, but with estimators $\what \gamma_i^{(n)}$. So stopping the algorithm at this point is optimal. We want to use hypothesis testing to determine $k$.
\begin{theorem}
Suppose $k \in \mathbb{N}$ and we want to test the null hypothesis $H_0:$ «$\gamma_1^{(n)} = 0$ for all $n \geq k$» against the alternative $H_a:$ There exist an $n \geq k$ such that $\gamma_1^{(n)} \neq 0$. Then 
\[
Z_k = \sum_{i=k}^{d-1} \what \rho_1^{(i)}n^{(i)} \approx \chi^2_{d-k}
\]
is a test observator with upper tailed rejection region.
\end{theorem}

Suppose we we choose $\alpha = 0.99$ The simplest scheme would be the following:

Add flow chart. As you can imagine the quality of the estimate depends on the number of observations per time constant $\tau$ of the correlation structure. Just like any method which attempts to compute the variance of the mean based on the covariances or variance, such as the blocking method, or a brute force estimation
\[
\widehat{\Var{\overline X}} = 2^{-d} \bigg[ \what\gamma_0^{(0)} + 2 \sum_{t=1}^{2^d - 1} \left( 1 - \frac{t}{n} \right)\what \gamma_t^{(0)} \bigg].
\]
In fact, we shall see in the validation of the method that the quality of the estimate is what we would expect, it improves exponentially as a function of $n/\tau$.
\subsection*{\uppercase{Validation of method}}

\begin{figure}[!htbp]

	\input{/home/marius/Dokumenter/master/blocking/bilder/fig1.tex}
	
  	\caption{Relative error squared of two autoregressive models versus observations per time autocorrelation time-constant. There is exponential convergence rate for two vastly different correlation structures. AR(1) autocorrelation is positive with exponential decay whilst AR(2) is oscillatory with exponential decay. The processes were distributed vastly different. They were Gamma(1,1) and Standard Normal respectively. The method was insensitive to the distribution of the observations ($p = 0.895$) [sjekk dette]. It is apparant that the first digit of the method was correct for $n = 50\tau$, and two digits correct for $n = 500\tau$. The mean relative errors $\mu$ were modelled by gamma regression, $\log \mu = \beta_0 + \beta_1 x$. Deviance explained was 50.65\% and 65.39\% respectively on 6078 degrees of freedom. Dashed lines give 95\% confidence intervals of the means.} \label{fig:figure1}
  	
\end{figure}

\subsection*{\uppercase{Proof of claims}}
The remaining results develop the mathematics of the blocking method. Let's start with a soft result on which we will build more structure.
\begin{prop*}
Suppose $d > 1$ is an integer and $X_1,\cdots, X_{m}$ are $m = 2^d$ random variables from a weakly stationary sample. If we let $\overline{X_m} = (1/m) \sum_{i=1}^m X_i$ denote the mean, and for $t = |i-j|$ we let $\gamma_{t}^{(k)} = \gamma_{i,j}^{(k)}$ be the autocovariance subject to $k$ blocking-transformations. Then the block transformed variables 
$X_1^{(k)}, \cdots , X_{2^{-k}m}^{(k)}$ are weakly stationary and 
\begin{equation}
\Var{\overline X_m} - \frac{1}{n^{(k)}}\Var{X^{(k)}_i} = \frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}-1} \left( 1 - \frac{t}{n^{(k)}} \right) \gamma_t^{(k)} \label{eq:epsilonM}
\end{equation}
and
\begin{equation}
\gamma_{j}^{(k)} = \ccases{ \frac{1}{2}\gamma_{2j}^{(k-1)} + \frac{1}{2}\gamma_{2j+1}^{(k-1)} & \qquad \text{if $j = 0$} \\
\frac{1}{4}\gamma_{2j-1}^{(k-1)} + \frac{1}{2}\gamma_{2j}^{(k-1)} + \frac{1}{4}\gamma_{2j+1}^{(k-1)} & \qquad \text{if $j > 0$}
} \label{eq:TM}
\end{equation}
\end{prop*}
\begin{proof}
See \cite{flyvbjerg_error_1989} or the appendix.
\end{proof}
The right hand side of equation \eqref{eq:epsilonM} represent the difference between the quantity which we want to estimate, and the estimator which we will use. Therefore, at blocking iteration number $k$, we will call the right hand side the \defn{error}, and denote it by $\varepsilon_k$. Next let's make a couple of simple observations
\begin{lemma*} Suppose $m = 2^d$, $d > 1$ denotes the number of observations and $k$ denote the number of blocking transformations. Then
\[
n^{(k)} = 2^{-k}m \qquad \text{and} \qquad k < \log_2m \qquad \text{for all} \qquad k<d.
\]\label{lemma:k}
\end{lemma*}
\begin{proof}
We proof the first part by induction. Suppose $k=0$. By definition, $n^{(0)} = m$ and so
\[
n^{(0)} = m = 2^{-0} m,
\]
Suppose now that there exist some $k \in \{0,1,2,\cdots\}$ such that $n^{(k)} = 2^{-k}m$ and $n^{(k+1)}$ exist, this is possible since $d > 1$ by hypothesis. By construction each blocking transformation halve the number of observations, so $n^{(k+1)} = n^{(k)}/2$ and therefore
\[
n^{(k+1)} = \frac{n^{(k)}}{2} = \frac{2^{-k}m}{2} = 2^{-(k+1)}m.
\]
To see the last observation is true, just observe that if $k < d$ and $m = 2^d$, then $k < d = d\log_2 2 = \log_2 m$.
\end{proof}
\begin{prop*}
Suppose $2^d$ is the number of observations, and $\gamma^{(k)}_0$ is finite for all $k \in \{ 0,1,\cdots, d-1 \}$. Then the rate of change of the error is
\begin{equation}
\varepsilon_k - \varepsilon_{  k+1} = \frac{\gamma_1^{(k)} }{n^{(k)}} \qquad \text{for all} \qquad 0 \leq k < d-1 \label{eq:rate}.
\end{equation}
\label{prop:diff}
\end{prop*}
\begin{proof}
We prove formula \eqref{eq:rate}, but first we define a functional to make notation more convenient. We will manipulate sums and will be interested in which function $f(t)$ appear in the terms $\gamma^{(k)}_{f(t)}$ inside the summation. Suppose $a \in \mathbb{Z}$ and define the polynoms $p_{a}(x) = 2x-a$ and a functional $S_{p_a(j)} : \{p_{-1},p_{0},p_{1}\} \to \mathbb{R}$ by: 
\begin{equation}
S_{p_a(j)} \equiv \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \gamma^{(k)}_{p_a(t)}. \label{eq:functional}
\end{equation}
That means, using $n^{(k+1)} = n^{(k)}/2$ , we can rewrite $\varepsilon_{  k+1}$ as
\begin{align}
\varepsilon_{  k+1} &\stackrel{\eqref{eq:epsilon}}{=} \frac{2}{n^{(k+1)}} \sum_{t=1}^{n^{(k+1)}-1} \left( 1 - \frac{t}{n^{(k+1)}} \right) \gamma^{(k+1)}_t \stackrel{\eqref{eq:T}}{=} \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \left( \gamma^{(k)}_{2t - 1} + 2\gamma^{(k)}_{2t} + \gamma^{(k)}_{2t + 1}\right) \nonumber \\
& \stackrel{\eqref{eq:functional}} {=} S_{2j-1} + 2S_{2j} + S_{2j+1} = 2(S_{2j-1} + S_{2j}) + S_{2j+1} - S_{2j-1} \label{eq:epsilonkpp}
\end{align}
where we added and subtracted $S_{2j - 1}$ in the last step. Write out the sums explicitly to obtain:
\begin{align}
S_{2j-1} &= \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \gamma^{(k)}_{2t - 1}  = \frac{1}{n^{(k)}}\sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t-1}{n^{(k)}}\right) \gamma^{(k)}_{2t - 1} - \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \frac{\gamma^{(k)}_{2t - 1}}{n^{(k)}} \label{eq:sumB}
\\
S_{2j+1} &= \frac{1}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}} \right) \gamma^{(k)}_{2t + 1} \nonumber  \\
&=  \frac{1}{n^{(k)}} \left[ \sum_{t=1}^{n^{(k)/2}-1}\gamma_{2t+1}^{(k)} - \left(\frac{2}{n^{(k)}} \gamma^{(k)}_{3} + \frac{4}{n^{(k)}} \gamma^{(k)}_{5} + \cdots + \frac{n^{(k)} - 2}{n^{(k)}} \gamma^{(k)}_{n^{k} - 1} \right)\right]\nonumber
 \\
 &= \frac{1}{n^{(k)}}\sum_{t=2}^{n^{(k)}/2} \left( 1 - \frac{2(t-1)}{n^{(k)}} \right) \gamma^{(k)}_{2t-1} = \frac{1}{n^{(k)}} \sum_{t=2}^{n^{(k)}/2} \left( 1 - \frac{2t-1}{n^{(k)}} \right) \gamma^{(k)}_{2t - 1} + \frac{1}{n^{(k)}}\sum_{t=2}^{n^{(k)}/2} \frac{\gamma^{(k)}_{2t - 1}}{n^{(k)}}  \nonumber
\end{align}
If the last term is subtracted from the first, we obtain after a few manipulations
%\begin{equation}
%S_{2j+1} - S_{2j-1} = \frac{2}{n^{(k)}} \sum_{t=2}^{n^{(k)}/2} \frac{1}{n^{(k)}} %\gamma^{(k)}_{2t - 1} - \frac{1}{n^{(k)}}\left( 1 - \frac{1}{n^{(k)}} \right) %\gamma^{(k)}_{1} \label{eq:differencesum}
%\end{equation}
\begin{equation}
S_{2j+1} - S_{2j-1} = \frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} - \frac{1}{n^{(k)}} \gamma^{(k)}_{1} \label{eq:differencesum}
\end{equation}
Lets also investigate the following quantity
\begin{align}
n^{(k)}(S_{2j-1} + S_{2j}) \stackrel{\eqref{eq:sumB} }{=} \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t-1}{n^{(k)}}\right) \gamma^{(k)}_{2t - 1} -  \sum_{t=1}^{n^{(k)}/2-1} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} + \sum_{t=1}^{n^{(k)}/2-1} \left( 1 - \frac{2t}{n^{(k)}}\right) \gamma^{(k)}_{2t}\nonumber\\
=\left[ \left(1 + \frac{1}{n^{(k)}}\right)\gamma^{(k)}_1 + \left(1 + \frac{2}{n^{(k)}}\right)\gamma^{(k)}_2  + \cdots + \left(1 + \frac{n^{(k)} - 2}{n^{(k)}} \right)\gamma^{(k)}_{n^{(k)} -2 }\right]-  \sum_{t=1}^{n^{(k)}/2-1} \frac{\gamma^{(k)}_{2t - 1}}{n^{(k)}} \nonumber\\
= \sum_{t=1}^{n^{(k)} -2 } \left( 1 - \frac{t}{n^{(k)}} \right)\gamma^{(k)}_{t}-  \sum_{t=1}^{n^{(k)}/2-1} \frac{\gamma^{(k)}_{2t - 1}}{n^{(k)}} = \sum_{t=1}^{n^{(k)} -1 } \left( 1 - \frac{t}{n^{(k)}} \right)\gamma^{(k)}_{t} -  \sum_{t=1}^{n^{(k)}/2} \frac{\gamma^{(k)}_{2t - 1}}{n^{(k)}} \label{eq:nsum}
\end{align}
In the last equality we added and subtracted $(1-t/n^{(k)})\gamma^{(k)}_{t}$ and in addition used that for $t = n^{(k)} - 1$, $(1-t/n^{(k)})\gamma^{(k)}_{t} = 1/n^{(k)}\gamma^{(k)}_{n^{(k)} -1}$. This means that if we consider
\begin{align}
2(S_{2j-1} + S_{2j}) \stackrel{\eqref{eq:nsum}}{=} \frac{2}{n^{(k)}} \left[ \sum_{t=1}^{n^{(k)} -1 } \left( 1 - \frac{t}{n^{(k)}} \right)\gamma^{(k)}_{t} -  \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} \right] = \varepsilon_k -\frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} \label{eq:sumsum}
\end{align}
Now substitute \eqref{eq:differencesum} and \eqref{eq:sumsum} into \eqref{eq:epsilonkpp}
\begin{align*}
\varepsilon_{  k+1} &\stackrel{\eqref{eq:epsilonkpp}}{=} 2(S_{2j-1} + S_{2j}) + S_{2j+1} - S_{2j-1} \\
&\stackrel{\eqref{eq:differencesum} \eqref{eq:sumsum}}{=} \varepsilon_k -\frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1}  + \frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}/2} \frac{1}{n^{(k)}} \gamma^{(k)}_{2t - 1} - \frac{1}{n^{(k)}} \gamma^{(k)}_{1}  = \varepsilon_k - \frac{\gamma^{(k)}_1 }{n^{(k)}}
\end{align*}
Subtract $\varepsilon_{  k+1} - \gamma^{(k)}_1/n^{(k)}$ from each side of the equation, and the proposition follows.
\end{proof}
The proposition says that the rate of convergence of the method is only dependant on the correlation between sequential observations $X_i^{(k)}$, and not the full correlation structure. But the following two corollaries show that the proposition prove the expected behaviour of the blocking method.
\begin{corollary*}
Suppose $2^d$ is the number of observations, $i,j\in \mathbb{N}$ and $\gamma^{(k)}_0$ is finite for all $k \in \{ 0,1,\cdots, d-1 \}$;
\begin{enumerate}
\item if there exist $k \in \mathbb{N}$ such that $\gamma^{(k)}_1 > 0$ for all $i \leq k \leq j$, then the sequence of errors $\varepsilon_k$ is strictly decreasing for all $j \leq k \leq i$.
\item if there exist $k \in \mathbb{N}$ such that $\gamma^{(k)}_1 \geq 0$ for all $i \leq k \leq j$, then the sequence of errors $\varepsilon_k$ is nonstrictly decreasing for all $j \leq k \leq i$.
\item if there exist some $i \in \mathbb{N}$ such that $\gamma^{(i)}_1 = 0$, and $\gamma^{(i)}_t \geq \gamma^{(i)}_{t+1}$ then the sequence of errors $\varepsilon_k$ is constant for all $i \leq k < d-1$.
\end{enumerate}
\end{corollary*}
\begin{proof}
\textit{1}. Suppose there exist $k \in \mathbb{N}$ such that $\gamma^{(k)}_1 > 0$ for all $i \leq k \leq j$. That means proposition \ref{prop:diff} is true. Suppose $u,v \in \mathbb{N}$ are distinct natural numbers such that $i \leq v \leq k \leq u \leq j$ and notice that since $n^{(k)} > 0$ and $\gamma^{(k)}_1 > 0$ is true by hypothesis, then the sum of such terms must be positive, and therefore
\begin{align*}
0 < \sum_{k = v}^{u-1} \frac{\gamma^{(k)}_1 }{n^{(k)}} &= \frac{\gamma^{(v)}_1}{n^{(v)}} + \frac{\gamma^{(v+1)}_1}{n^{(v+1)}} + \cdots + \frac{\gamma^{(u-1)}_1}{n^{(u-1)}} \stackrel{\eqref{eq:rate}}{=} (\varepsilon_v - \varepsilon_{v+1}) + (\varepsilon_{v+1} - \varepsilon_{v+2}) + \cdots + (\varepsilon_{u-1} - \varepsilon_{u}) \\
&= \varepsilon_v - \varepsilon_{u},
\end{align*}
since every term of the sum cancel except from the first and the last. Now, by adding $\varepsilon_u$ to each side of the inequality, the first part is proved. \\
\\
\textit{2}. To obtain the second part of the Corollary, replace the strict inequalities with inequalities.\\
\\
\textit{3}. To see the last point, suppose there exist some $i \in \mathbb{N}$ such that $\gamma^{(i)}_1 = 0$ and $\gamma^{(i)}_t \geq \gamma^{(i)}_{t+1}$. Since proposition \ref{prop:diff} is true,
\[
\varepsilon_i - \varepsilon_{i+1} = \frac{\gamma_1^{(i)}}{n^{(i)}} = 0
\]
We use induction to see that $\varepsilon_{i+b} - \varepsilon_{i+b+1} = 0$ for $b > 0$:
\begin{align*}
\varepsilon_{i+1} - \varepsilon_{i+2} &= \frac{\gamma_1^{(i+1)}}{n^{(i+1)}} = \frac{1}{n^{(i+1)}} \frac{1}{4} \left( \gamma^{(i)}_{1} + 2 \gamma^{(i)}_{2} + \gamma^{(i)}_{3} \right) \\
&\leq \frac{1}{n^{(i+1)}} \frac{1}{4} \left( \gamma^{(i)}_{1} + 2 \gamma^{(i)}_{1} + 2 \gamma^{(i)}_{1} \right) = \frac{1}{n^{(i+1)}} \frac{1}{4} \left( 0+0+0 \right) = 0.
\end{align*}
\end{proof}

\begin{corollary*}
If the blocked variables become independent, for some number of blocking transformation, then the sequence of errors $\varepsilon_k$ become constant.
\end{corollary*}
\begin{proof}
Use the previous corollary, and notice that $\gamma^{(k)}_1 = 0$ if $X_i,X_{i+1}$ are independent for all $0 \leq i,j \leq 2^{-k}m$.
\end{proof}

\begin{prop*}
Suppose $X_1,X_2,\cdots,X_n$ are weakly stationary observations, then subject to $i$ blocking transformations,
\[
\E{ \what \gamma_1^{(i)} - \gamma_1^{(i)} } = \Var{\overline{X}_n}\left( 1 - \frac{2n^{(i)} }{n^{(i)} - 1} \right) + \frac{1}{n^{(i)}}\frac{2}{n^{(i)} - 1} \sum_{t=0}^{n^{(i)} - 1} \gamma_t^{(i)}, \qquad \text{for all $i \in \{0,1,\cdots,d-1\}$}
\]
Suppose $X_1^i, X_2^i, \cdots X_n^i$ is uncorrelated, then
\[
\E{ \what \gamma_1^{(i)} } = -\Var{\overline{X}_n} \frac{n^i + 1}{n^i -1} + \frac{2}{n^{(i)}(n^{(i)}-1) } \gamma^{(i)}_0
\]
\end{prop*}

\begin{prop}
\[
4 \sum_{t=1}^{n^{(i)} - 1} t \gamma_t^{(i)} = \sum_{t=1}^{n^{(i-1)} - 1} t \gamma_t^{(i-1)} - n_{i} \gamma_{n_{i-1} - 1}^{(i-1)}
\]
\[
2\sum_{t=1}^{n^{(i)} - 1} \gamma_t^{(i)} = \sum_{t=1}^{n^{(i-1)} - 1} \gamma_t^{(i-1)} - \frac{1}{2} \left( \gamma_{1}^{(i-1)} + \gamma_{n_{i-1} - 1}^{(i-1)} \right)
\]
\end{prop}

\begin{lemma}
Suppose $j$ and $k$ are positive natural number and the sample size $m \geq 2^k(j-1) + 2^{k+1} - 1$, then $k < \log_2 m$ and
\begin{align}
\gamma_{j}^{(k)} &= 2^{-2k} \Big[ \gamma_{2^k(j-1)+1}^{(0)} + 2\gamma_{2^k(j-1)+2}^{(0)} +3\gamma_{2^k(j-1)+3}^{(0)} + \cdots + 2^k\gamma_{2^k(j-1)+2^k}^{(0)} \nonumber \\
&+ (2^k -1) \gamma_{2^k(j-1)+2^k + 1}^{(0)} + (2^k -2) \gamma_{2^k(j-1)+2^k + 2}^{(0)} + \cdots + \gamma_{2^k(j-1)+2^{k+1} - 1}^{(0)} \Big] . \label{eq:hypothesis}
\end{align}
\end{lemma}
\begin{proof}
We first show that $k < \log_2 m$. Fix $j$ and $k$ such that $m \geq 2^k(j-1) + 2^{k+1} - 1$, then
\[
m \geq \undercbrace{2^k(j-1)}_{\geq 0} + \undercbrace{2^{k+1} - 1}_{\geq 2^k +1} \geq 2^k + 1 \qquad \text{only if} \qquad \log_2m \geq \log_2(2^k + 1) > \log_2 2^k = k \log_2 2 = k
\]
We prove the rest of the lemma by induction. Fix $j$ such that $m \geq 2^1(j-1) + 2^{1+1} - 1$, in particular this ensures that if $k = 1$, then $m \geq 2j + 1$ and therefore $\gamma_{2j+1}^{(0)}$ exists. Define $M = \sup_{k \in \mathbb{N}} \{m \geq 2^k(j-1) + 2^{k+1} - 1\}$. Assume $k=1$ and write
\[
\gamma_{j}^{(1)} \stackrel{\eqref{eq:T}}{=} 2^{-2} \left( \gamma_{2j-1}^{(0)} + \gamma_{2j}^{(0)} + \gamma_{2j+1}^{(0)} \right)
\]
Assume now that there exist a positive natural number $k < M$ such that equation \eqref{eq:hypothesis} is true. This implies $k+1 \leq M$ and hence $\gamma_{2^k2j+2^k+2^k-1}^{(0)}$ exists, and we can write
\begin{align*} 
\gamma_{j}^{(k+1)} \stackrel{\eqref{eq:T}}{=} &2^{-2} (\gamma_{  k,2j-1} + 2\gamma_{  k,2j} + \gamma_{  k,2j+1}) \stackrel{\eqref{eq:hypothesis}}{=} \\
&2^{-2} 2^{-2k}\Big( \gamma_{0,2^k(2j-2)+1} + 2\gamma_{0,2^k(2j-2)+2} + \cdots + 2^k\gamma_{0,2^k(2j-2)+2^k} \\
&+ (2^k-1)\gamma_{0,2^k(2j-2)+2^k+1} + \cdots + \gamma_{0,2^k(2j-2)+2^k+2^k-1}
\Big) + \\
&2^{-2} 2^{-2k}\Big( 2\gamma_{0,2^k(2j-1)+1} + 4\gamma_{0,2^k(2j-1)+2} + \cdots + 2\,2^k\gamma_{0,2^k(2j-1)+2^k} \\
&+ 2(2^k-1)\gamma_{0,2^k(2j-1)+2^k+1} + \cdots + 2\gamma_{0,2^k(2j-1)+2^k+2^k-1}
\Big) + \\
&2^{-2} 2^{-2k}\Big( \gamma_{0,2^k2j+1} + 2\gamma_{0,2^k2j+2} + \cdots + 2^k\gamma_{0,2^k2j +2^k} \\
&+ (2^k-1)\gamma_{0,2^k2j+2^k+1} + \cdots + \gamma_{0,2^k2j+2^k+2^k-1}
\Big).
\end{align*}
By using that $2^{-2} 2^{-2k} = 2^{-2(k+1)}$ and factoring $\gamma_{K}^{(0)}$ together for all $2^{k+1}(j-1) + 1 \leq K \leq 2^{k+1}(j-1) + 2^{k+2}-1$, the lemma follows.
\end{proof}
The next proposition is useful because combined with proposition \ref{prop:diff}, it shows how the convergence of the method is uniquely determined by the initial correlation structure.
\begin{prop}
Suppose $\big(\gamma_1^{(k)} , \gamma_2^{(k)}, \cdots, \gamma_{n_k}^{(k)}\big)$ denote the correlation structure at blocking iteration number $k$. Suppose $m \geq 2^{k+1} - 1$, then $k < \log_2 m$ and,
\[
2^{  2k} \gamma_1^{(k)} = \gamma_1^{(0)} + 2\gamma_2^{(0)} + 3 \gamma_3^{(0)} + \cdots +2^k \gamma_{2^k}^{(0)} + (2^k-1)\gamma_{2^k+1}^{(0)} + \cdots + \gamma_{2^{k+1}-1}^{(0)}
\] \label{prop:sequence}
\end{prop}
\begin{proof}
Use the previous lemma with $j=1$.
\end{proof}
Forklar ideen i neste bevis. Vis noen ligninger
\begin{lemma}
Bla bla
\[
\E{\what \sigma^2} = \Var{X_i} - \Var{ \overline X_n} 
\]
 \label{prop:sigmahat}
\end{lemma}
\begin{proof}
To come
\end{proof}
Introduser ideen ved å vise hvordan vi trekker fra hverandre ligningene.
\begin{theorem}[Revised blocking method]
Suppose $X_1,\cdots, X_n$ are $n = 2^d$ weakly stationary random variables. Let $\overline X_n = (1/n)\sum_{j=1}^n X_j$ denotes their average, $\sigma^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2$ and $\gamma^{(j)}_1$ be a the autocovariance of subsequent random variable subject to $j$ blocking transformations, then
\[
\Var{\overline{X}_n} = \frac{1}{n-1} \bigg( \E{\what \sigma^2} + \sum_{i=0}^{d-1}2^i \gamma^{(i)}_1  \bigg).
\]
\end{theorem}
\textit{Proof to come.} Did not have time to finish it before the exams. Furthermore, the proof appear to be more difficult than first anticipated. If there is interest in it, I can try to work out the details after the exams finish.\\
\\
This theorem says that after less than $\log_2 m$ blocking iterations, the difference between the variance of the mean and variance of the blocked variables, is essentially zero. This is the case since $m$ is typically many orders of magnitude larger than $\Var{X^{(k)}_i}$.
\subsection*{\uppercase{Estimation}}
Bla bla Forklar hvorfor man gjør det. Vis at den er effektiv
\section*{\uppercase{Conclusion and perspectives}}
Bla bla
\section*{\uppercase{Appendix}}
\begin{prop*}
Suppose $d > 1$ is an integer and $X_1,\cdots, X_{m}$ are $m = 2^d$ random variables from a weakly stationary sample. If we let $\overline{X} = (1/m) \sum_{i=1}^m X_i$ denote the mean, and for $t = |i-j|$ we let $\gamma_{t}^{(k)} = \gamma_{i,j}^{(k)}$ be the autocovariance subject to $k$ blocking-transformations. Then the block transformed variables 
$X_1^{(k)}, \cdots , X_{2^{-k}m}^{(k)}$ are weakly stationary, 
\begin{equation}
\Var{\overline X_m} - \frac{1}{n^{(k)}}\Var{X^{(k)}_i} = \frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)}-1} \left( 1 - \frac{t}{n^{(k)}} \right) \gamma_t^{(k)} \label{eq:epsilon}
\end{equation}
and
\begin{equation}
\gamma_{j}^{(k)} = \ccases{ \frac{1}{2}\gamma_{2j}^{(k-1)} + \frac{1}{2}\gamma_{2j+1}^{(k-1)} & \qquad \text{if $j = 0$} \\
\frac{1}{4}\gamma_{2j-1}^{(k-1)} + \frac{1}{2}\gamma_{2j}^{(k-1)} + \frac{1}{4}\gamma_{2j+1}^{(k-1)} & \qquad \text{if $j > 0$}
} \label{eq:T}
\end{equation}
\end{prop*}
\begin{proof}
We first show that $X_1^{(k)}, \cdots , X_{2^{-k}m}^{(k)}$ are weakly stationary. We prove each of criteria by induction. By hypothesis, for all $i \in \{1,\cdots,m\}$ there exist some $\mu \in \mathbb{R}$ such that $X_i = \mu$ since the sample is weakly stationary. Therefore, for all $i \in \{1,2,\cdots,2^0m\}$, $ \E{X^{(0)}_i } = \mu$. The induction step follows by 
\[
\E{X^{(k+1)}_i } \stackrel{\eqref{eq:block}}{=} \frac{1}{2} \E{ X^{(k)}_{2i-1} + X^{(k)}_{2i} } = \mu
\]
For the second criterion, notice that $\gamma_{i,j}^{(0)} = \gamma_{|i-j|}^{(0)}$ since the sample is weakly stationary. To see that for all $n \in \mathbb{N}$ the autocovariance $\gamma_{i,j}$ only depends on $i,j$ through the difference $|i-j|$, perform the induction step by 
\begin{align*}
\gamma_{u,v}^{(k+1)} &= \cov \left(X_{u}^{(k+1)}, X_{v}^{(k+1)} \right) \stackrel{\eqref{eq:block}}{=}  \cov\left( \frac{1}{2} ( X_{2u-1}^{(k)} + X_{2u}^{(k)} ) , \frac{1}{2} ( X_{2v-1}^{(k)} + X_{2v}^{(k)} ) \right) \\
&= \frac{1}{4} \left( \cov \left(X_{2u}^{(k)}, X_{2v}^{(k)} \right) + \cov \left(X_{2u-1}^{(k)}, X_{2v-1}^{(k)} \right) + \cov \left(X_{2u-1}^{(k)}, X_{2v}^{(k)} \right) + \cov \left(X_{2u}^{(k)}, X_{2v-1}^{(k)} \right) \right)\\
&= \ccases{
\gamma_0^{(k)}/2 + \gamma_1^{(k)}/2 & \quad \text{if $|u-v| = 0$} \\
\gamma^{(k)}_{2|u-v|-1}/4 + \gamma^{(k)}_{2|u-v|}/2 + \gamma^{(k)}_{2|u-v|+1}/4 & \quad \text{if $|u-v| > 0$}.
}
\end{align*}
This formula proves equation \eqref{eq:T} proves that the sample $X_{i}^{(k)}$ is weakly stationary for all $k \in \{0,1,\cdots,d-1\}$. It remains to prove the formula \eqref{eq:epsilon}. Suppose we let $\overline{X}^{(k)}_{n^{(k)}}$ denote the mean subject to $k$ blocking transformations, then
\begin{align}
\Var{\overline{X}^{(k)}_{n^{(k)}}} &\stackrel{\eqref{eq:varcov} }{=} \E{{\overline{X}^{(k)}_{n^{(k)}}}^2} - \E{{\overline{X}^{(k)}_{n^{(k)}}}}^2 \nonumber \\
&= \frac{1}{(n^{(k)})^2}\E{  \left(\sum_{i=1}^{n^{(k)}}X_i^{(k)} \right)\left(\sum_{j=1}^{n^{(k)}}X_j^{(k)} \right) } - \frac{1}{(n^{(k)})^2}\E{\sum_{i=1}^{n^{(k)}}X_i^{(k)}} \E{\sum_{j=1}^{n^{(k)}}X_j^{(k)}}  \nonumber \\
&= \frac{1}{(n^{(k)})^2} \sum_{i=1}^{n^{(k)}} \left[\E{X_i^{(k)} X_j^{(k)}} - \E{X_i^{(k)}}\E{X_j^{(k)}} \right] = \frac{1}{(n^{(k)})^2} \sum_{i=1}^{n^{(k)}} \sum_{j=1}^{n^{(k)}} \gamma_{i,j}^{(k)} \label{eq:errorestimate}
\end{align}
Recall that the sample subject to $k$ blocking transformations is weakly stationary, that means that the autocovariance only depends on its indecies $i,j$ though the difference $|i-j| \equiv t$. And since $|i-j| = |j-i|$ it follows that the covariance matrix $\Sigma^{(k)}$ satisfies
\begin{equation}
\Sigma^{(k)} = \begin{bmatrix}
\gamma^{(k)}_{1,1} & \gamma^{(k)}_{1,2} & \cdots & \gamma^{(k)}_{1,n^{(k)}} \\
\gamma^{(k)}_{2,1} & \gamma^{(k)}_{2,2} & \cdots &  \\
\vdots & & \ddots \\
\gamma^{(k)}_{n^{(k)},1} & \gamma^{(k)}_{n^{(k)},2} & \cdots & \gamma^{(k)}_{n^{(k)},n^{(k)}}
\end{bmatrix} = 
\begin{bmatrix}
\gamma^{(k)}_{0} & \gamma^{(k)}_{1} & \cdots & \gamma^{(k)}_{n^{(k)}-1} \\
\gamma^{(k)}_{1} & \gamma^{(k)}_{0} & \cdots &  \\
\vdots & & \ddots \\
\gamma^{(k)}_{n^{(k)}-1} & \gamma^{(k)}_{n^{(k)}-2} & \cdots & \gamma^{(k)}_{0}
\end{bmatrix} \label{eq:subdiagonal}
\end{equation}

We want to evaluate the double sum of equation \eqref{eq:errorestimate}, which is the 
sum of all the terms in $\Sigma^{(k)}$, but since by equation \eqref{eq:subdiagonal}, $\Sigma^{(k)}$ is $n-$diagonal, we can rewrite the double sum as a single sum over each of its diagonal like so:
\begin{align*}
\Var{\overline{X}^{(k)}_{n^{(k)}}} &\stackrel{\eqref{eq:errorestimate} }{=} \frac{1}{(n^{(k)})^2} \sum_{i=1}^{n^{(k)}} \sum_{j=1}^{n^{(k)}} \gamma_{i,j}^{(k)}  \stackrel{\eqref{eq:subdiagonal} }{=} \frac{1}{(n^{(k)})^2} \sum_{t=0}^{n^{(k)} - 1}  (n^{(k)} - t) \gamma_{t}^{(k)} \\
&= \frac{\gamma_0^{(k)}}{n^{(k)}} + \frac{2}{n^{(k)}} \sum_{t=1}^{n^{(k)} - 1} \left( 1 - \frac{t}{n^{(k)}} \right)\gamma^{(k)}_t
\end{align*}
That means the only thing which remain is to show is $\Var{\overline{X}^{(k)}_{n^{(k)}}} = \Var{\overline{X}_{m}}$, which follows by induction using
\begin{align*}
\overline{X}^{(k+1)}_{n^{(k+1)}} &= \frac{1}{n^{(k+1)}} \sum_{i=1}^{n^{(k+1)}}X_i^{(k+1)} = \frac{1}{n^{(k)}/2} \sum_{i=1}^{n^{(k)}/2} \frac{1}{2} \left( X^{(k)}_{2i-1} + X^{(k)}_{2i} \right) \\
&= \left( X_1^{(k)} + X_3^{(k)} + \cdots X_{n^{(k)} - 1}^{(k)} + X_2^{(k)} + X_4^{(k)} + \cdots X_{n^{(k)}}^{(k)} \right) = \overline{X}^{(k)}_{n^{(k)}}.
\end{align*}
This proves formula \eqref{eq:epsilon}, and the proposition follows.
\end{proof}

\printbibliography
\end{document}